{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c626a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is word wise tokenization-: \n",
      " ['CSI-DYPIEMR', 'is', 'the', 'Student', 'Chapter', 'of', 'Computer', 'Society', 'of', 'India', 'in', 'Dr.', 'D.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'Computer', 'Society', 'of', 'India', 'is', 'a', 'body', 'of', 'computer', 'professionals', 'in', 'India', '.', 'It', 'was', 'started', 'on', '6', 'March', '1965', 'by', 'a', 'few', 'computer', 'professionals', 'and', 'has', 'now', 'grown', 'to', 'be', 'the', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', 'has', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', 'and', '100,000', 'members', '.'] \n",
      "\n",
      "x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x \n",
      "\n",
      "This is sentence wise tokenization-: \n",
      " [\"CSI-DYPIEMR is the Student Chapter of Computer Society of India in Dr. D. Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering, Management, and Research.\", 'Computer Society of India is a body of computer professionals in India.', 'It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals.', 'It has 72 chapters across India, 511 student branches, and 100,000 members.']\n"
     ]
    }
   ],
   "source": [
    "#Dividing a block/body of text into words or sentences is known as tokenization\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "block = \"CSI-DYPIEMR is the Student Chapter of Computer Society of India in Dr. D. Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering, Management, and Research. Computer Society of India is a body of computer professionals in India. It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals. It has 72 chapters across India, 511 student branches, and 100,000 members.\"\n",
    "\n",
    "print(\"This is word wise tokenization-:\",'\\n', nltk.word_tokenize(block), '\\n')\n",
    "\n",
    "print(\"x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x\", '\\n')\n",
    "\n",
    "print(\"This is sentence wise tokenization-:\",'\\n', nltk.sent_tokenize(block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558a0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above example we have used two methods (word_tokenize & sent_tokenize) from the nltk library to demonstrate\n",
    "# word wise tokenization and sentence wise tokenization. The punkt module is downloaded to aid in the recognition of\n",
    "# punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a8e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version-: \n",
      " ['CSI-DYPIEMR', 'is', 'the', 'Student', 'Chapter', 'of', 'Computer', 'Society', 'of', 'India', 'in', 'Dr.', 'D.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'Computer', 'Society', 'of', 'India', 'is', 'a', 'body', 'of', 'computer', 'professionals', 'in', 'India', '.', 'It', 'was', 'started', 'on', '6', 'March', '1965', 'by', 'a', 'few', 'computer', 'professionals', 'and', 'has', 'now', 'grown', 'to', 'be', 'the', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', 'has', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', 'and', '100,000', 'members', '.'] \n",
      "\n",
      "x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x \n",
      "\n",
      "This is the cleaned version-: \n",
      " ['CSI-DYPIEMR', 'Student', 'Chapter', 'Computer', 'Society', 'India', 'Dr.', 'D.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'Computer', 'Society', 'India', 'body', 'computer', 'professionals', 'India', '.', 'It', 'started', '6', 'March', '1965', 'computer', 'professionals', 'grown', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', '100,000', 'members', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords are those types of words that don't have much meaningful contribution in the sentence like \"a\", \"it's\",\n",
    "# \"is\", \"the\" etc. Presence of large amount of these types of words act as a form of noise in the dataset, that's why\n",
    "# their removal is important\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "# This function contains the entire list of stop words present inside different languages, for our use case, we'll\n",
    "#focus on english stopwords\n",
    "\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "        \n",
    "print(\"This is the unclean version-:\",'\\n',  token, '\\n')\n",
    "\n",
    "print(\"x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x-o-x\", '\\n')\n",
    "\n",
    "print(\"This is the cleaned version-:\",'\\n', cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f86f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentence may contain words that convey the same meaning but are written in different forms, taking for example \n",
    "# verbs in different forms of tenses like 'running', 'ran', 'run', 'runs' ultimately convey the same meaning but \n",
    "# are written in different forms to suit the different types of tenses, for computer analysis these types of words \n",
    "# are kept under same section as their base form (which in this case will be \"run\") and this process is known as stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86b70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain', 'rain', 'rain', 'rain']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = ['rain', 'rained', 'raining', 'rains']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa9423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CSI-DYPIEMR', 'JJ'), ('Student', 'NNP'), ('Chapter', 'NNP'), ('Computer', 'NNP'), ('Society', 'NNP'), ('India', 'NNP'), ('Dr.', 'NNP'), ('D.', 'NNP'), ('Y.', 'NNP'), ('Patil', 'NNP'), ('Pratishthan', 'NNP'), (\"'s\", 'POS'), ('Dr.', 'NNP'), ('D.', 'NNP'), ('Y.', 'NNP'), ('Patil', 'NNP'), ('Institute', 'NNP'), ('Engineering', 'NNP'), (',', ','), ('Management', 'NNP'), (',', ','), ('Research', 'NNP'), ('.', '.'), ('Computer', 'NNP'), ('Society', 'NNP'), ('India', 'NNP'), ('body', 'NN'), ('computer', 'NN'), ('professionals', 'NNS'), ('India', 'NNP'), ('.', '.'), ('It', 'PRP'), ('started', 'VBD'), ('6', 'CD'), ('March', 'NNP'), ('1965', 'CD'), ('computer', 'NN'), ('professionals', 'NNS'), ('grown', 'VBP'), ('national', 'JJ'), ('body', 'NN'), ('representing', 'VBG'), ('computer', 'NN'), ('professionals', 'NNS'), ('.', '.'), ('It', 'PRP'), ('72', 'CD'), ('chapters', 'NNS'), ('across', 'IN'), ('India', 'NNP'), (',', ','), ('511', 'CD'), ('student', 'NN'), ('branches', 'NNS'), (',', ','), ('100,000', 'CD'), ('members', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(cleaned_token)     \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a9e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization is the process of finding the form of the related word in the dictionary. Lemmatization does not \n",
    "# simply chop off inflections. Instead, it uses lexical knowledge bases to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb7ed91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSI-DYPIEMR', 'Student', 'Chapter', 'Computer', 'Society', 'India', 'Dr.', 'D.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'Computer', 'Society', 'India', 'body', 'computer', 'professional', 'India', '.', 'It', 'started', '6', 'March', '1965', 'computer', 'professional', 'grown', 'national', 'body', 'representing', 'computer', 'professional', '.', 'It', '72', 'chapter', 'across', 'India', ',', '511', 'student', 'branch', ',', '100,000', 'member', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04278bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF_IDF PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81544fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474b71f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer', 'sections', 'ensures', 'a', 'good', 'backgrounds', 'various', 'The', 'making', 'projects', 'of', 'professionals.', 'work', 'technical', 'works', 'promotion', 'each', 'at', 'together.', 'interest', 'where', 'Technology', 'organizes', 'awards.', 'To', 'conventions,', 'is', 'organized', 'priority', 'the', 'And', 'top', 'today.', 'towards', 'projects,', 'with', 'also', 'same', 'on', 'amongst', 'all', 'training', 'develop', 'IT', 'in', 'conferences,', 'from', 'objective,', 'choice', 'Keeping', 'that', 'among', 'come', 'teach,', 'it', 'society.', 'for', 'enthusiasts,', 'together', 'updating', 'future', 'Information', 'regular', 'culture', 'fulfill', 'an', 'other', 'skill', 'mind', 'and', 'regularly', 'time,', 'aim', 'Our', 'guide', 'collaborate', 'grow', 'are', 'CSI', 'students,', 'as', 'to', 'lectures,', 'profession', 'this', 'area', 'students', 'professionals'}\n"
     ]
    }
   ],
   "source": [
    "block_1 = \"Our aim is to develop a good work culture among students, a culture where students from various technical backgrounds come together to teach, guide and collaborate with each other on various projects and grow together.\"\n",
    "\n",
    "block_2 = \"Keeping in mind the interest of the IT professionals and computer enthusiasts, CSI works towards making the profession an area of choice amongst all sections of the society. The promotion of Information Technology as a profession is the top priority of CSI today. To fulfill this objective, the CSI regularly organizes conferences, conventions, lectures, projects, and awards. And at the same time, it also ensures that regular training and skill updating are organized for the future IT professionals.\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "\n",
    "#join them to remove common duplicate words\n",
    "total= set(first_block).union(set(second_block))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e42edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "\n",
    "for word in first_block:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_block:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72e4b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>computer</th>\n",
       "      <th>sections</th>\n",
       "      <th>ensures</th>\n",
       "      <th>a</th>\n",
       "      <th>good</th>\n",
       "      <th>backgrounds</th>\n",
       "      <th>various</th>\n",
       "      <th>The</th>\n",
       "      <th>making</th>\n",
       "      <th>projects</th>\n",
       "      <th>...</th>\n",
       "      <th>CSI</th>\n",
       "      <th>students,</th>\n",
       "      <th>as</th>\n",
       "      <th>to</th>\n",
       "      <th>lectures,</th>\n",
       "      <th>profession</th>\n",
       "      <th>this</th>\n",
       "      <th>area</th>\n",
       "      <th>students</th>\n",
       "      <th>professionals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   computer  sections  ensures  a  good  backgrounds  various  The  making  \\\n",
       "0         0         0        0  2     1            1        2    0       0   \n",
       "1         1         1        1  1     0            0        0    1       1   \n",
       "\n",
       "   projects  ...  CSI  students,  as  to  lectures,  profession  this  area  \\\n",
       "0         1  ...    0          1   0   2          0           0     0     0   \n",
       "1         0  ...    3          0   1   0          1           2     1     1   \n",
       "\n",
       "   students  professionals  \n",
       "0         1              0  \n",
       "1         0              1  \n",
       "\n",
       "[2 rows x 88 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2855488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69e5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'sections', 'ensures', 'good', 'backgrounds', 'various', 'The', 'making', 'projects', 'professionals.', 'work', 'technical', 'works', 'promotion', 'together.', 'interest', 'Technology', 'organizes', 'awards.', 'To', 'conventions,', 'organized', 'priority', 'And', 'top', 'today.', 'towards', 'projects,', 'also', 'amongst', 'training', 'develop', 'IT', 'conferences,', 'objective,', 'choice', 'Keeping', 'among', 'come', 'teach,', 'society.', 'enthusiasts,', 'together', 'updating', 'future', 'Information', 'regular', 'culture', 'fulfill', 'skill', 'mind', 'regularly', 'time,', 'aim', 'Our', 'guide', 'collaborate', 'grow', 'CSI', 'students,', 'lectures,', 'profession', 'area', 'students', 'professionals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now we'll remove stopwords from the list\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bffb861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now moving onto the IDF Part\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)   \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N / (float(val) + 1))     \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1c7b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   computer  sections   ensures         a      good  backgrounds   various  \\\n",
      "0  0.000000  0.000000  0.000000  0.017202  0.008601     0.008601  0.017202   \n",
      "1  0.003859  0.003859  0.003859  0.003859  0.000000     0.000000  0.000000   \n",
      "\n",
      "        The    making  projects  ...       CSI  students,        as        to  \\\n",
      "0  0.000000  0.000000  0.008601  ...  0.000000   0.008601  0.000000  0.017202   \n",
      "1  0.003859  0.003859  0.000000  ...  0.011578   0.000000  0.003859  0.000000   \n",
      "\n",
      "   lectures,  profession      this      area  students  professionals  \n",
      "0   0.000000    0.000000  0.000000  0.000000  0.008601       0.000000  \n",
      "1   0.003859    0.007719  0.003859  0.003859  0.000000       0.003859  \n",
      "\n",
      "[2 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now we'll implement the IDF formula\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items(): tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdc718b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above way was the generic/formulatic way of implementing TFIDF, This process can be made way more simpler by using\n",
    "# sklearn library, example given below\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Make sure all words are in lowercase\n",
    "\n",
    "version_1 = \"Developing a competitive culture where the students polish technical and professional attributes, gain experience and learn new skills while upgrading the already present skillset. For those fledglings who have a zeal to build a strong profile and are hunting for their Ikigai, CSI provides ample opportunities for those individuals too.\"\n",
    "version_2 = \"Personalized career guidance, Regular Logic and aptitude building activities, Industrial level project collaboration, Building a network with active collaborations across the globe, Periodic member exclusive conferences and seminars, Created a community for sharing skills and knowledge\"\n",
    "\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([version_1.lower(), version_2.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a3eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 61)\t0.13915271943780658\n",
      "  (0, 31)\t0.13915271943780658\n",
      "  (0, 40)\t0.13915271943780658\n",
      "  (0, 4)\t0.13915271943780658\n",
      "  (0, 48)\t0.13915271943780658\n",
      "  (0, 18)\t0.13915271943780658\n",
      "  (0, 30)\t0.13915271943780658\n",
      "  (0, 58)\t0.13915271943780658\n",
      "  (0, 29)\t0.13915271943780658\n",
      "  (0, 7)\t0.13915271943780658\n",
      "  (0, 46)\t0.13915271943780658\n",
      "  (0, 54)\t0.13915271943780658\n",
      "  (0, 9)\t0.13915271943780658\n",
      "  (0, 60)\t0.13915271943780658\n",
      "  (0, 67)\t0.13915271943780658\n",
      "  (0, 28)\t0.13915271943780658\n",
      "  (0, 65)\t0.13915271943780658\n",
      "  (0, 23)\t0.13915271943780658\n",
      "  (0, 59)\t0.27830543887561315\n",
      "  (0, 24)\t0.2970249178760062\n",
      "  (0, 53)\t0.13915271943780658\n",
      "  (0, 44)\t0.13915271943780658\n",
      "  (0, 3)\t0.13915271943780658\n",
      "  (0, 62)\t0.13915271943780658\n",
      "  (0, 64)\t0.13915271943780658\n",
      "  :\t:\n",
      "  (1, 21)\t0.16649349332910351\n",
      "  (1, 37)\t0.16649349332910351\n",
      "  (1, 41)\t0.16649349332910351\n",
      "  (1, 26)\t0.16649349332910351\n",
      "  (1, 0)\t0.16649349332910351\n",
      "  (1, 13)\t0.16649349332910351\n",
      "  (1, 1)\t0.16649349332910351\n",
      "  (1, 66)\t0.16649349332910351\n",
      "  (1, 38)\t0.16649349332910351\n",
      "  (1, 12)\t0.16649349332910351\n",
      "  (1, 47)\t0.16649349332910351\n",
      "  (1, 35)\t0.16649349332910351\n",
      "  (1, 32)\t0.16649349332910351\n",
      "  (1, 2)\t0.16649349332910351\n",
      "  (1, 10)\t0.33298698665820703\n",
      "  (1, 6)\t0.16649349332910351\n",
      "  (1, 36)\t0.16649349332910351\n",
      "  (1, 49)\t0.16649349332910351\n",
      "  (1, 27)\t0.16649349332910351\n",
      "  (1, 11)\t0.16649349332910351\n",
      "  (1, 42)\t0.16649349332910351\n",
      "  (1, 24)\t0.11846149176425531\n",
      "  (1, 52)\t0.11846149176425531\n",
      "  (1, 5)\t0.35538447529276596\n",
      "  (1, 57)\t0.11846149176425531\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33488cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
